{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import utils\n",
    "from collections import OrderedDict, Counter\n",
    "import ast\n",
    "import swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_itemized = pd.read_json(\"xandr_segments_itemized.json\") # load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "type",
     "evalue": "unterminated string literal (detected at line 1) (<unknown>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSyntaxError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m eu_countries[\u001b[39m\"\u001b[39m\u001b[39mcodes\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m eu_countries[\u001b[39m\"\u001b[39m\u001b[39mcodes\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(ast\u001b[39m.\u001b[39mliteral_eval)\n\u001b[1;32m      7\u001b[0m eu_countries[\u001b[39m\"\u001b[39m\u001b[39mstrings\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m eu_countries[\u001b[39m\"\u001b[39m\u001b[39mstrings\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(ast\u001b[39m.\u001b[39mliteral_eval)\n\u001b[0;32m----> 8\u001b[0m eu_countries[\u001b[39m\"\u001b[39m\u001b[39midentifiers\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m eu_countries[\u001b[39m\"\u001b[39;49m\u001b[39midentifiers\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(ast\u001b[39m.\u001b[39;49mliteral_eval)\n\u001b[1;32m      9\u001b[0m eu_names_re \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(np\u001b[39m.\u001b[39mconcatenate(eu_countries[[\u001b[39m\"\u001b[39m\u001b[39mstrings\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39midentifiers\u001b[39m\u001b[39m\"\u001b[39m]]\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mflatten()))\n\u001b[1;32m     10\u001b[0m eu_codes \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(np\u001b[39m.\u001b[39mconcatenate(eu_countries[\u001b[39m\"\u001b[39m\u001b[39mcodes\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mflatten()))\n",
      "File \u001b[0;32m~/Desktop/xandr/.venv/lib/python3.10/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/Desktop/xandr/.venv/lib/python3.10/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/Desktop/xandr/.venv/lib/python3.10/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1077\u001b[0m             values,\n\u001b[1;32m   1078\u001b[0m             f,\n\u001b[1;32m   1079\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1080\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Desktop/xandr/.venv/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ast.py:62\u001b[0m, in \u001b[0;36mliteral_eval\u001b[0;34m(node_or_string)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39mSafely evaluate an expression node or a string containing a Python\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mexpression.  The string or node provided may only consist of the following\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mPython literal structures: strings, bytes, numbers, tuples, lists, dicts,\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39msets, booleans, and None.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(node_or_string, \u001b[39mstr\u001b[39m):\n\u001b[0;32m---> 62\u001b[0m     node_or_string \u001b[39m=\u001b[39m parse(node_or_string\u001b[39m.\u001b[39;49mlstrip(\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m\"\u001b[39;49m), mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39meval\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(node_or_string, Expression):\n\u001b[1;32m     64\u001b[0m     node_or_string \u001b[39m=\u001b[39m node_or_string\u001b[39m.\u001b[39mbody\n",
      "File \u001b[0;32m/usr/lib/python3.10/ast.py:50\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(source, filename, mode, type_comments, feature_version)\u001b[0m\n\u001b[1;32m     48\u001b[0m     feature_version \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[39m# Else it should be an int giving the minor version for 3.x.\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcompile\u001b[39;49m(source, filename, mode, flags,\n\u001b[1;32m     51\u001b[0m                _feature_version\u001b[39m=\u001b[39;49mfeature_version)\n",
      "\u001b[0;31mSyntaxError\u001b[0m: unterminated string literal (detected at line 1) (<unknown>, line 1)"
     ]
    }
   ],
   "source": [
    "eu_countries = pd.read_csv(\"eu_countries.csv\")\n",
    "\n",
    "eu_countries[\"names_re\"] = np.nan\n",
    "eu_countries[\"code_set\"] = np.nan\n",
    "\n",
    "eu_countries[\"codes\"] = eu_countries[\"codes\"].apply(ast.literal_eval)\n",
    "eu_countries[\"strings\"] = eu_countries[\"strings\"].apply(ast.literal_eval)\n",
    "eu_countries[\"identifiers\"] = eu_countries[\"identifiers\"].apply(ast.literal_eval)\n",
    "eu_names_re = \"|\".join(np.concatenate(eu_countries[[\"strings\", \"identifiers\"]].values.flatten()))\n",
    "eu_codes = set(np.concatenate(eu_countries[\"codes\"].values.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_count = Counter(np.concatenate(df_itemized[\"name_list\"].values))\n",
    "with open(\"tags.json\", \"w\") as f:\n",
    "    json.dump(OrderedDict(tag_count.most_common()), f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7ed1040ecc4e0aa9ee48c0a503c285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/648930 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "travel_word_list = [\"travel\", \"departure\", \"destination\",\n",
    "                    \"tourism\", \"tourist\", \"vacation\", \"holiday\", \"voyage\", \"expedia\"]\n",
    "\n",
    "eu_false_positives = [\"(furniture|nail) polish\",  # ethnicity FPs\n",
    "                      \"irish (whiskey|cream)\",\n",
    "                      \"speak(er|ing)\",\n",
    "                      # \"language\",\n",
    "                      # \"hispanic\",\n",
    "                      \"tour de france\",\n",
    "                      \"greek joghurt\",\n",
    "\n",
    "                      # Country code FPs\n",
    "                      \"accuen\",  # does market research and very little location-specifics. Thus many FPs\n",
    "                      \"xaxisus\",  # us source\n",
    "                      \"xaxisca\",  # canadian source\n",
    "                      \"xaxisapc\",  # whatever the fuck it is, it's not useful\n",
    "                      \"tailtarget\",  # mostly latAm focussed -> numerous es/pt FPs\n",
    "                      \"\\A\\d{20}\",\n",
    "                      ]\n",
    "\n",
    "\n",
    "false_positive_re = \"|\".join(travel_word_list + eu_false_positives)\n",
    "filtered_travel_words = df_itemized[df_itemized.swifter.apply(\n",
    "    lambda x: not re.search(false_positive_re, x[\"name\"]), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1821d1612ad6411dbcaad8875f4329a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/622147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "us_re = r\"united[ -–_]?states|usa\"\n",
    "\n",
    "def filter_us_names(row):\n",
    "    # does the full country name occur anywhere or does a countrycode match an item exactly?\n",
    "    return bool(re.search(us_re, row[\"name\"]) or any([el == \"usa\" or el.startswith(\"us \") or el.endswith(\" us\") for el in row[\"name_list\"]]))\n",
    "\n",
    "filtered_us = filtered_travel_words[filtered_travel_words.swifter.apply(filter_us_names, axis=1)]\n",
    "filtered_us.to_csv(\"filtered_us.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ce687329d94e4ba16bae3d626c2e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/609261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def filter_eu_names(row):\n",
    "    # does the full country name occur anywhere or does a countrycode match an item exactly?\n",
    "    return bool(re.search(eu_names_re, row[\"name\"]) or len(eu_codes.intersection(row[\"name_list\"])))\n",
    "\n",
    "def filter_eu_codes(row):\n",
    "    # does the full country name occur anywhere or does a countrycode match an item exactly?\n",
    "    return len(eu_codes.intersection(row[\"name_list\"])) > 0\n",
    "\n",
    "eu_ethnicities_re = \"|\".join([x[1] for x in eu_countries[\"strings\"].values])\n",
    "\n",
    "\n",
    "def filter_eu_ethnicities(row):\n",
    "    # does the full country name occur anywhere or does a countrycode match an item exactly?\n",
    "    return bool(re.search(eu_ethnicities_re, row[\"name\"]))\n",
    "\n",
    "\n",
    "filtered_eu = filtered_travel_words[filtered_travel_words.swifter.apply(filter_eu_names, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_eu.to_csv(\"filtered_eu.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86dacdbbf704942b7f5e2a290c754ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/622147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf9585d76a441f8a7a75deb36518b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/622147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_travel_words[filtered_travel_words.swifter.apply(filter_eu_codes, axis=1)].to_csv(\"filtered_eu_codes.csv\")\n",
    "filtered_travel_words[filtered_travel_words.swifter.apply(filter_eu_ethnicities, axis=1)].to_csv(\"filtered_eu_ethnicities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"filtered_eu_segnames.json\", \"w\") as f:\n",
    "    json.dump(list(filtered_eu[\"name\"].array), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spicy_dict = {word:{} for word in spicy_words}\n",
    "for index, row in filtered_eu.iterrows():\n",
    "    segname = row[\"name\"]\n",
    "    provider = row[\"provider_name\"]\n",
    "    res = re.findall(spicy_word_re, segname)\n",
    "    if res:\n",
    "        res = set(res)\n",
    "        for tag in res:\n",
    "            if provider in spicy_dict[tag]:\n",
    "                spicy_dict[tag][provider].append(segname)\n",
    "            else:\n",
    "                spicy_dict[tag][provider] = [segname]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for k,v in spicy_dict.items():\n",
    "    for k2, v2 in v.items():\n",
    "        v[k2] = sorted(v2)\n",
    "\n",
    "\n",
    "with open(\"spicy_words_eu.json\", \"w\") as f:\n",
    "    json.dump(spicy_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spicy_dict_global = {word:{} for word in spicy_words}\n",
    "for index, row in df_itemized.iterrows():\n",
    "    segname = row[\"name\"]\n",
    "    provider = row[\"provider_name\"]\n",
    "    res = re.findall(spicy_word_re, segname)\n",
    "    if res:\n",
    "        res = set(res)\n",
    "        for tag in res:\n",
    "            if provider in spicy_dict_global[tag]:\n",
    "                spicy_dict_global[tag][provider].append(segname)\n",
    "            else:\n",
    "                spicy_dict_global[tag][provider] = [segname]\n",
    "\n",
    "\n",
    "for k,v in spicy_dict_global.items():\n",
    "    for k2, v2 in v.items():\n",
    "        v[k2] = sorted(v2)\n",
    "    # spicy_dict[k] = sorted(v)\n",
    "\n",
    "\n",
    "with open(\"spicy_words_global.json\", \"w\") as f:\n",
    "    json.dump(spicy_dict_global, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = eu_countries[\"name\"].values\n",
    "strings = eu_countries[\"strings\"].values\n",
    "codes = eu_countries[\"codes\"].values\n",
    "identifiers = eu_countries[\"identifiers\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_counter = {country: [] for country in countries}\n",
    "\n",
    "country_re = [\"|\".join(strings[i]+identifiers[i]) for i in range(len(strings))]\n",
    "\n",
    "for index, row in filtered_travel_words.iterrows():\n",
    "    for i, country in enumerate(countries):\n",
    "        segname = row[\"name\"]\n",
    "        \n",
    "        if re.search(country_re[i], segname) or codes[i] in row[\"name_list\"]:\n",
    "            eu_counter[country].append(segname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "germany 4849\n",
      "spain 3528\n",
      "france 3371\n",
      "italy 2288\n",
      "denmark 1953\n",
      "sweden 1398\n",
      "europe 1037\n",
      "poland 544\n",
      "finland 507\n",
      "austria 477\n",
      "belgium 455\n",
      "romania 406\n",
      "netherlands 374\n",
      "hungary 268\n",
      "portugal 211\n",
      "greece 155\n",
      "ireland 117\n",
      "czechia 98\n",
      "croatia 28\n",
      "malta 25\n",
      "bulgaria 18\n",
      "slovakia 18\n",
      "estonia 15\n",
      "cyprus 15\n",
      "slovenia 14\n",
      "lithuania 13\n",
      "latvia 12\n",
      "luxembourg 11\n"
     ]
    }
   ],
   "source": [
    "counts = OrderedDict([(k, len(v)) for k,v in eu_counter.items()])\n",
    "for a, b in sorted(counts.items(), key= lambda x: x[1], reverse=True):\n",
    "    print(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22205"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([i[1] for i in counts.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
